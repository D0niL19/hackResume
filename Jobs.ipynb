{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "K-tQ9rCzwEw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "c6KInKTzvc2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь подкачиваются данные с гугл диска для дальнейшей работы"
      ],
      "metadata": {
        "id": "XcXTmoF5xgNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1YHLbtohCZ4NxHeDMN0iDweV0ota7tQPC\n",
        "!unzip train_dataset_vprod_train.zip"
      ],
      "metadata": {
        "id": "yTPNDwBDv9Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Попробуем прочитать файл в режиме бинарного чтения и декодировать с обработкой ошибок\n",
        "with open('vprod_train/JOB_LIST.csv', 'rb') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Попробуем декодировать содержимое с заменой ошибок\n",
        "decoded_content = content.decode('utf-8', errors='replace')\n",
        "\n",
        "# Сохранение декодированных данных во временный файл для последующего чтения\n",
        "with open('temp.csv', 'w', encoding='utf-8') as temp_file:\n",
        "    temp_file.write(decoded_content)\n",
        "\n",
        "# Чтение данных из временного файла\n",
        "df = pd.read_csv('temp.csv')\n"
      ],
      "metadata": {
        "id": "2IS0e1-mv9Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "9XdBJDO4vjUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для фильтрации только русских, английских букв и пробелов\n",
        "def preprocess_text(text):\n",
        "    # Оставляем только русские, английские буквы и пробелы\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'\\d+', '', text)\n",
        "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Применяем предобработку ко всем строкам в колонке 'job_title'\n",
        "df['job_title'] = df['job_title'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "52PDFDidwnf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df.sample(n=100000, random_state=42) #Случайная выборка 100 тысяч образцов"
      ],
      "metadata": {
        "id": "7I_ru8wswsGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Векторизация профессий\n",
        "vectorizer = TfidfVectorizer(analyzer='char')\n",
        "X = vectorizer.fit_transform(sampled_df['job_title'])"
      ],
      "metadata": {
        "id": "gyVUaVvcwtVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Кластеризация\n",
        "kmeans = KMeans(n_clusters=10000, random_state=42)\n",
        "sampled_df['cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Обобщение профессий\n",
        "def get_general_profession(cluster):\n",
        "    cluster_data = sampled_df[sampled_df['cluster'] == cluster]\n",
        "    return cluster_data['lemmatized_profession'].mode()[0]\n",
        "\n",
        "sampled_df['general_profession'] = sampled_df['cluster'].apply(get_general_profession)\n",
        "# Замена в датафрейме\n",
        "sampled_df['final_profession'] = sampled_df['general_profession']"
      ],
      "metadata": {
        "id": "NWBrdboLw3RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация модели и токенизатора\n",
        "model_name = 'gpt2'  # Вы можете выбрать другую модель, если хотите\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Функция для запроса к модели\n",
        "def get_common_profession(cluster_examples):\n",
        "    prompt = (\n",
        "        \"Собраны примеры профессий из одного кластера:\\n\"\n",
        "        + \"\\n\".join(cluster_examples) + \"\\n\"\n",
        "        + \"Пожалуйста, объедините их в одно общее название профессии:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=100,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split('\\n')[-1].strip()  # Возвращаем последний ответ после новой строки\n",
        "\n",
        "# Сбор примеров из каждого кластера и получение объединенных названий профессий\n",
        "clustered_professions = sampled_df.groupby('cluster')['job_title'].apply(list).to_dict()\n",
        "new_professions = []\n",
        "\n",
        "for cluster, examples in clustered_professions.items():\n",
        "    # Получаем 5 примеров для каждого кластера\n",
        "    cluster_examples = examples[:5]\n",
        "    common_profession = get_common_profession(cluster_examples)\n",
        "    new_professions.append({\n",
        "        'cluster': cluster,\n",
        "        'common_profession': common_profession\n",
        "    })\n",
        "\n",
        "# Создание нового DataFrame\n",
        "new_df = pd.DataFrame(new_professions)\n",
        "\n",
        "# Сохранение нового DataFrame в CSV\n",
        "new_df.to_csv('common_professions.csv', index=False)\n",
        "\n",
        "# Вывод результата\n",
        "print(new_df)\n"
      ],
      "metadata": {
        "id": "I8_vJPEJGpsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}